{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "367fe927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a7c251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.nn.Embedding(4,8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c145578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.5442, -0.3583,  0.4284, -0.5506, -0.6946, -2.0262, -1.6451, -0.7431],\n",
       "        [-0.3511, -1.7066, -0.2856,  0.6253, -0.8071,  0.9050, -0.0854,  0.5278],\n",
       "        [ 1.1195,  0.3508,  1.0625, -0.1631,  1.9232,  0.1876, -1.9495,  0.9537],\n",
       "        [ 0.1406, -0.7225,  0.2274,  0.8747, -2.0024,  2.4018, -0.2076, -1.6268]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs.weight\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "873e6d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5442, -0.3583,  0.4284, -0.5506, -0.6946, -2.0262, -1.6451, -0.7431],\n",
       "        [-0.3511, -1.7066, -0.2856,  0.6253, -0.8071,  0.9050, -0.0854,  0.5278],\n",
       "        [ 1.1195,  0.3508,  1.0625, -0.1631,  1.9232,  0.1876, -1.9495,  0.9537],\n",
       "        [ 0.1406, -0.7225,  0.2274,  0.8747, -2.0024,  2.4018, -0.2076, -1.6268]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs.data\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2298d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create weight matrices and set dimensions\n",
    "d_in = 8\n",
    "d_out = 6\n",
    "W_q = torch.nn.Parameter(torch.rand((d_in,d_out)), requires_grad=False)\n",
    "W_k = torch.nn.Parameter(torch.rand((d_in,d_out)), requires_grad=False)\n",
    "W_v = torch.nn.Parameter(torch.rand((d_in,d_out)), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d46faf0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9045, 1.8137, 2.1289, 2.3067, 2.6276, 0.5842])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#choose input vector and transform it into our query vector using W_q\n",
    "\n",
    "query = inputs[2] @W_q\n",
    "query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23202195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: tensor([[-2.3055, -3.5374, -3.0642, -2.1090, -1.8208, -1.4988],\n",
      "        [-0.3400, -0.3018, -0.0149, -0.8319, -2.2124, -1.4911],\n",
      "        [ 1.5205,  1.4319,  2.3963,  0.9467,  1.4426,  1.3657],\n",
      "        [-1.8230, -1.0397,  0.7561, -0.0270, -1.7616, -0.3677]])\n",
      "Values: tensor([[-2.3307, -2.1374, -1.8553, -1.1769, -2.9780, -3.3913],\n",
      "        [-0.3609, -0.0309, -0.0068, -1.1789,  0.7904, -0.6430],\n",
      "        [ 2.0029,  0.8580,  1.2310,  2.1924,  2.7329,  2.0493],\n",
      "        [-0.9863, -0.1657,  1.3662, -1.3847,  0.9532, -1.9188]])\n"
     ]
    }
   ],
   "source": [
    "#calcualte attention scores using keys generated by w_k\n",
    "keys = inputs @ W_k\n",
    "values = inputs @ W_v\n",
    "print(\"Keys:\", keys)\n",
    "print(\"Values:\", values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd53d716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.0591, 9.4606, 7.5099, 1.1274])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = query @ keys.T\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a71a7d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2018, 0.5378, 0.2425, 0.0179])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = torch.softmax( attention_scores / keys.shape[-1]**0.5, dim=-1 )\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9a213154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions_weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b671baa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1963, -0.2427, -0.0549, -0.3646,  0.5041, -0.5674])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector = attentions_weights @ values\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "795b6805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "13bda980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention( nn.Module ):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        #create weight matrices\n",
    "        self.W_q = nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "        self.W_k = nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "        self.W_v = nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "\n",
    "    # x = embedding vectors (inputs)\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_q\n",
    "        keys = x @ self.W_k\n",
    "        values = x @ self.W_v\n",
    "        scores = queries @ keys.T\n",
    "        weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "        context = weights @ values\n",
    "        return context; \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "98699ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple = SimpleAttention(d_in = 8, d_out = 6,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b6ce7735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0431,  0.0146, -1.2971, -0.0171,  0.5555, -0.1975],\n",
       "        [-1.4581, -1.0632,  0.9727, -0.2306,  0.6323, -1.5716],\n",
       "        [ 1.8000,  0.6147,  0.3825, -1.7541, -0.5908,  1.5765],\n",
       "        [-1.1288,  1.3772,  0.3967, -1.1890,  1.9831,  1.4136],\n",
       "        [-0.6202, -0.6380,  1.3297, -0.9203,  0.0260,  1.4321],\n",
       "        [-1.0993,  0.9146,  0.1289, -0.6392, -1.6315,  0.2007],\n",
       "        [-0.8788, -0.5630, -1.0449, -1.1407,  0.6796,  0.4080],\n",
       "        [-1.9095,  0.2027, -0.0195,  0.3786,  0.5853,  0.5494]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple.W_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d496f4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.5676,  0.1781, -1.1646,  3.9914, -0.4402, -2.4747],\n",
       "        [-0.0714, -4.0302,  2.7484, -3.6080, -4.0779,  0.7696],\n",
       "        [-2.8627, -0.3220, -0.8772,  3.0581, -0.9525, -1.7089],\n",
       "        [ 0.8136, -4.3467,  3.2426, -5.0195, -3.7475,  1.0389]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = simple(inputs)\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cdb2d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second version but its linear not rand\n",
    "\n",
    "class SimpleAttentionv2( nn.Module ):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        #create weight matrices\n",
    "        self.W_q = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_k = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_v = nn.Linear( d_in, d_out, bias=False )\n",
    "\n",
    "\n",
    "    # x = embedding vectors (inputs)\n",
    "    def forward( self, x ):\n",
    "        queries = self.W_q( x )\n",
    "        keys = self.W_k( x )\n",
    "        values = self.W_v( x )\n",
    "        scores = queries @ keys.T\n",
    "        weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "        context = weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a4e6c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple = SimpleAttentionv2(d_in = 8, d_out=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b1496f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5182, -0.4823, -0.2079, -0.1227, -0.2912,  0.0909],\n",
       "        [ 0.6335, -0.0115,  0.0521, -0.3479, -0.1974, -0.1220],\n",
       "        [ 0.4956, -0.4126, -0.1995, -0.1590, -0.2194,  0.0902],\n",
       "        [ 0.5541,  0.0658,  0.1989, -0.3611, -0.1316, -0.0659]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = simple(inputs)\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "74d7f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.rand(inputs.shape[0], inputs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7ab7a1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4148, 0.3903, 0.3606, 0.7087],\n",
       "        [0.5272, 0.3624, 0.2373, 0.4305],\n",
       "        [0.3355, 0.2843, 0.4658, 0.1366],\n",
       "        [0.1564, 0.0350, 0.8193, 0.7746]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "85d8e864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8743, 1.5573, 1.2223, 1.7852])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84b185",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bae70628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_mask = torch.tril(torch.ones(weights.shape[0],weights.shape[0]))\n",
    "simple_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aa0bbf73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4148, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5272, 0.3624, 0.0000, 0.0000],\n",
       "        [0.3355, 0.2843, 0.4658, 0.0000],\n",
       "        [0.1564, 0.0350, 0.8193, 0.7746]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights = weights*simple_mask\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dab7865e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4148, 0.8896, 1.0857, 1.7852])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights.sum(dim= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ec8a2c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4148],\n",
       "        [0.8896],\n",
       "        [1.0857],\n",
       "        [1.7852]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_sums = masked_weights.sum(dim=  -1, keepdim=True)\n",
    "row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "af766a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5927, 0.4073, 0.0000, 0.0000],\n",
       "        [0.3090, 0.2619, 0.4291, 0.0000],\n",
       "        [0.0876, 0.0196, 0.4589, 0.4339]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights = masked_weights/row_sums\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "038cace0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu( torch.ones(weights.shape[0], weights.shape[0]), diagonal=1 )\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "17fdaf87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [False, False,  True,  True],\n",
       "        [False, False, False,  True],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ef37f2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4148, 0.3903, 0.3606, 0.7087],\n",
       "        [0.5272, 0.3624, 0.2373, 0.4305],\n",
       "        [0.3355, 0.2843, 0.4658, 0.1366],\n",
       "        [0.1564, 0.0350, 0.8193, 0.7746]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "71b04134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4148,   -inf,   -inf,   -inf],\n",
       "        [0.5272, 0.3624,   -inf,   -inf],\n",
       "        [0.3355, 0.2843, 0.4658,   -inf],\n",
       "        [0.1564, 0.0350, 0.8193, 0.7746]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = weights.masked_fill( mask.bool(), -torch.inf)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5ae08eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5411, 0.4589, 0.0000, 0.0000],\n",
       "        [0.3237, 0.3075, 0.3688, 0.0000],\n",
       "        [0.1760, 0.1559, 0.3415, 0.3266]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights = torch.softmax(weights, dim=-1)\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "88f2e4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "91b5eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop out a technique to prevent overfitting and making a model too good for one task makes it super poor at anything else, dropout fixes this\n",
    "dropout = nn.Dropout(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "66189730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0884, -0.0000,  0.8569, -0.0000, -0.0000, -0.0000, -3.2903, -1.4862],\n",
       "        [-0.7022, -0.0000, -0.0000,  0.0000, -0.0000,  1.8101, -0.1707,  0.0000],\n",
       "        [ 2.2391,  0.7016,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  1.9074],\n",
       "        [ 0.0000, -0.0000,  0.4547,  1.7494, -0.0000,  4.8036, -0.4152, -0.0000]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b32a4b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = torch.stack((inputs,inputs), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "feed5642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d409380",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97774d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d00c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a78cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module): #handles batches of input\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        #create weight matrices\n",
    "        self.W_q = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_k = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_v = nn.Linear( d_in, d_out, bias=False )\n",
    "        # include dropout:\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "        # use the following to manage memory effeciently\n",
    "        self.register_buffer(\"mask\", torch.triu( torch.ones(context_length, context_length), diagonal = 1 ))\n",
    "        \n",
    "\n",
    "\n",
    "    # x = embedding vectors or our inputs\n",
    "    def forward( self, x ):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # b = batch size \n",
    "\n",
    "        queries = self.W_q( x )\n",
    "        keys = self.W_k( x )\n",
    "        values = self.W_v( x )\n",
    "        scores = queries @ keys.transpose(1, 2)\n",
    "        scores.masked_fill_( self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "        weights = self.dropout( weights )\n",
    "        context = weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cefd110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal = CausalAttention(d_in=8, d_out=6, context_length=4, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8cb6bc5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2801,  0.4566, -0.0922, -0.2678,  0.3117, -0.5622],\n",
       "         [-0.2931,  0.4365, -0.1526, -0.3974, -0.0170, -0.5062],\n",
       "         [-0.1752,  0.5183,  0.0115, -0.0622,  0.1382, -0.1060],\n",
       "         [-0.2423,  0.5500,  0.0260,  0.0285,  0.0581, -0.0713]],\n",
       "\n",
       "        [[-0.2801,  0.4566, -0.0922, -0.2678,  0.3117, -0.5622],\n",
       "         [-0.2931,  0.4365, -0.1526, -0.3974, -0.0170, -0.5062],\n",
       "         [-0.1752,  0.5183,  0.0115, -0.0622,  0.1382, -0.1060],\n",
       "         [-0.2423,  0.5500,  0.0260,  0.0285,  0.0581, -0.0713]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0b3e46a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q = nn.Linear( d_in, d_out, bias=False )\n",
    "W_k = nn.Linear( d_in, d_out, bias=False )\n",
    "W_v = nn.Linear( d_in, d_out, bias=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "431d1739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9158,  0.6004, -0.0566,  0.3280,  0.6197,  0.0381],\n",
       "         [-0.0827,  0.4487,  0.2674,  0.3950, -0.7260, -0.0990],\n",
       "         [ 0.7467,  0.6134, -0.0659,  0.1101,  1.4163, -0.4855],\n",
       "         [-0.1021,  0.3142, -0.4756,  0.1686, -0.8854, -0.0643]],\n",
       "\n",
       "        [[ 0.9158,  0.6004, -0.0566,  0.3280,  0.6197,  0.0381],\n",
       "         [-0.0827,  0.4487,  0.2674,  0.3950, -0.7260, -0.0990],\n",
       "         [ 0.7467,  0.6134, -0.0659,  0.1101,  1.4163, -0.4855],\n",
       "         [-0.1021,  0.3142, -0.4756,  0.1686, -0.8854, -0.0643]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = W_q(batches)\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d0b1fc42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7923, -0.9866, -1.1296, -0.7488, -0.1404,  0.3901],\n",
       "         [-0.7893,  0.3488, -0.0692,  0.5706, -0.1268,  0.2835],\n",
       "         [ 0.9018,  0.2469,  0.7944, -0.7284, -0.2210,  0.3315],\n",
       "         [-0.4690, -0.5637, -0.3125,  0.8002, -0.4247, -0.2924]],\n",
       "\n",
       "        [[ 0.7923, -0.9866, -1.1296, -0.7488, -0.1404,  0.3901],\n",
       "         [-0.7893,  0.3488, -0.0692,  0.5706, -0.1268,  0.2835],\n",
       "         [ 0.9018,  0.2469,  0.7944, -0.7284, -0.2210,  0.3315],\n",
       "         [-0.4690, -0.5637, -0.3125,  0.8002, -0.4247, -0.2924]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = W_k(batches)\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4b267ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7923,  0.7923],\n",
       "         [-0.7893, -0.7893],\n",
       "         [ 0.9018,  0.9018],\n",
       "         [-0.4690, -0.4690]],\n",
       "\n",
       "        [[-0.9866, -0.9866],\n",
       "         [ 0.3488,  0.3488],\n",
       "         [ 0.2469,  0.2469],\n",
       "         [-0.5637, -0.5637]],\n",
       "\n",
       "        [[-1.1296, -1.1296],\n",
       "         [-0.0692, -0.0692],\n",
       "         [ 0.7944,  0.7944],\n",
       "         [-0.3125, -0.3125]],\n",
       "\n",
       "        [[-0.7488, -0.7488],\n",
       "         [ 0.5706,  0.5706],\n",
       "         [-0.7284, -0.7284],\n",
       "         [ 0.8002,  0.8002]],\n",
       "\n",
       "        [[-0.1404, -0.1404],\n",
       "         [-0.1268, -0.1268],\n",
       "         [-0.2210, -0.2210],\n",
       "         [-0.4247, -0.4247]],\n",
       "\n",
       "        [[ 0.3901,  0.3901],\n",
       "         [ 0.2835,  0.2835],\n",
       "         [ 0.3315,  0.3315],\n",
       "         [-0.2924, -0.2924]]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "30f10be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention( nn.Module ):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)]\n",
    "        )\n",
    "    \n",
    "    def forward( self, x ):\n",
    "        return torch.cat( [ head(x) for head in self.heads ], dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f1bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention( d_in = 8, d_out = 6, context_length = 4, dropout = 0, num_heads = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "881ed06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_out = mha(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9cd1f590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0881,  1.4246,  0.6861, -0.7970,  0.5636, -0.0636,  0.4785,\n",
       "           0.3121, -0.8059, -1.5977,  0.5405,  0.4089, -0.0568,  0.9947,\n",
       "          -0.1470, -1.2102,  0.2025,  1.4195],\n",
       "         [-0.0697,  0.5218, -0.1794, -0.3795,  0.2088, -0.3289,  0.5855,\n",
       "           0.0450, -0.2163, -0.9563,  0.6335, -0.2039,  0.1436,  0.4795,\n",
       "          -0.0107, -0.6811, -0.0916,  0.5425],\n",
       "         [-0.0084,  0.7840,  0.5018, -0.3541,  0.2783, -0.1358,  0.1869,\n",
       "           0.3372, -0.8090, -0.3943,  0.1776, -0.1160, -0.1748,  0.3729,\n",
       "          -0.1810, -0.5420, -0.2024,  0.6769],\n",
       "         [ 0.1652, -0.0743, -0.1640, -0.0382,  0.0895, -0.3747,  0.2541,\n",
       "           0.2574, -0.7839, -0.6654,  0.2674, -0.0360,  0.1747,  0.1574,\n",
       "           0.2572, -0.2579, -0.1765,  0.3013]],\n",
       "\n",
       "        [[-0.0881,  1.4246,  0.6861, -0.7970,  0.5636, -0.0636,  0.4785,\n",
       "           0.3121, -0.8059, -1.5977,  0.5405,  0.4089, -0.0568,  0.9947,\n",
       "          -0.1470, -1.2102,  0.2025,  1.4195],\n",
       "         [-0.0697,  0.5218, -0.1794, -0.3795,  0.2088, -0.3289,  0.5855,\n",
       "           0.0450, -0.2163, -0.9563,  0.6335, -0.2039,  0.1436,  0.4795,\n",
       "          -0.0107, -0.6811, -0.0916,  0.5425],\n",
       "         [-0.0084,  0.7840,  0.5018, -0.3541,  0.2783, -0.1358,  0.1869,\n",
       "           0.3372, -0.8090, -0.3943,  0.1776, -0.1160, -0.1748,  0.3729,\n",
       "          -0.1810, -0.5420, -0.2024,  0.6769],\n",
       "         [ 0.1652, -0.0743, -0.1640, -0.0382,  0.0895, -0.3747,  0.2541,\n",
       "           0.2574, -0.7839, -0.6654,  0.2674, -0.0360,  0.1747,  0.1574,\n",
       "           0.2572, -0.2579, -0.1765,  0.3013]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "351b7761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 18])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1d3cdcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#more efficient and better resource allocation\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        \n",
    "        super().__init__() \n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads \n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    " \n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`, \n",
    "        # this will result in errors in the mask creation further below. \n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forward method.\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "007d8725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4ffbd649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5442, -0.3583,  0.4284, -0.5506, -0.6946, -2.0262, -1.6451,\n",
       "          -0.7431],\n",
       "         [-0.3511, -1.7066, -0.2856,  0.6253, -0.8071,  0.9050, -0.0854,\n",
       "           0.5278],\n",
       "         [ 1.1195,  0.3508,  1.0625, -0.1631,  1.9232,  0.1876, -1.9495,\n",
       "           0.9537],\n",
       "         [ 0.1406, -0.7225,  0.2274,  0.8747, -2.0024,  2.4018, -0.2076,\n",
       "          -1.6268]],\n",
       "\n",
       "        [[ 0.5442, -0.3583,  0.4284, -0.5506, -0.6946, -2.0262, -1.6451,\n",
       "          -0.7431],\n",
       "         [-0.3511, -1.7066, -0.2856,  0.6253, -0.8071,  0.9050, -0.0854,\n",
       "           0.5278],\n",
       "         [ 1.1195,  0.3508,  1.0625, -0.1631,  1.9232,  0.1876, -1.9495,\n",
       "           0.9537],\n",
       "         [ 0.1406, -0.7225,  0.2274,  0.8747, -2.0024,  2.4018, -0.2076,\n",
       "          -1.6268]]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3989fa6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.5442, -0.3583,  0.4284, -0.5506],\n",
       "          [-0.6946, -2.0262, -1.6451, -0.7431]],\n",
       "\n",
       "         [[-0.3511, -1.7066, -0.2856,  0.6253],\n",
       "          [-0.8071,  0.9050, -0.0854,  0.5278]],\n",
       "\n",
       "         [[ 1.1195,  0.3508,  1.0625, -0.1631],\n",
       "          [ 1.9232,  0.1876, -1.9495,  0.9537]],\n",
       "\n",
       "         [[ 0.1406, -0.7225,  0.2274,  0.8747],\n",
       "          [-2.0024,  2.4018, -0.2076, -1.6268]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5442, -0.3583,  0.4284, -0.5506],\n",
       "          [-0.6946, -2.0262, -1.6451, -0.7431]],\n",
       "\n",
       "         [[-0.3511, -1.7066, -0.2856,  0.6253],\n",
       "          [-0.8071,  0.9050, -0.0854,  0.5278]],\n",
       "\n",
       "         [[ 1.1195,  0.3508,  1.0625, -0.1631],\n",
       "          [ 1.9232,  0.1876, -1.9495,  0.9537]],\n",
       "\n",
       "         [[ 0.1406, -0.7225,  0.2274,  0.8747],\n",
       "          [-2.0024,  2.4018, -0.2076, -1.6268]]]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches.view(2,4,2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6704b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(d_in = 8, d_out = 6, context_length = 4, dropout = 0, num_heads = 3 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6f8d73e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_out = mha(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4be0c32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5516,  0.3211,  0.0186,  0.3213, -0.0403,  0.0474],\n",
       "         [ 0.2749, -0.1132, -0.1236,  0.4172,  0.0530,  0.0911],\n",
       "         [ 0.2293, -0.1545, -0.0346,  0.4729, -0.0608,  0.0936],\n",
       "         [ 0.1880, -0.3267, -0.1679,  0.5925,  0.1006,  0.1268]],\n",
       "\n",
       "        [[ 0.5516,  0.3211,  0.0186,  0.3213, -0.0403,  0.0474],\n",
       "         [ 0.2749, -0.1132, -0.1236,  0.4172,  0.0530,  0.0911],\n",
       "         [ 0.2293, -0.1545, -0.0346,  0.4729, -0.0608,  0.0936],\n",
       "         [ 0.1880, -0.3267, -0.1679,  0.5925,  0.1006,  0.1268]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "13a5eb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 6])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
