{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20ab6b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ac75e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_model import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0653ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "beb4164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open (\"thelostrace_instruction_data.json\", \"r\") as f:\n",
    "    instruction_data = json.load(f)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5653a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(entry):\n",
    "    text = (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    if entry[\"input\"]:\n",
    "        text += f\"\\n\\n### Input:\\n{entry['input']}\"\n",
    "    text += f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "    return text\n",
    "\n",
    "text_data = \"\\n\\n\".join(format_instruction(e) for e in instruction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e7f4e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10186fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/eval \n",
    "# separate text into training and validation sets:\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f25c2bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss helpers\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ee01589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#device - cuda\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24f70d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b195e98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\radcl\\AppData\\Local\\Temp\\ipykernel_44736\\2980432663.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\"gpt2-124M-thelostrace-sft.pth\", map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 6.888, Val loss 7.455\n",
      "Ep 1 (Step 000005): Train loss 3.644, Val loss 4.754\n",
      "Every effort moves you?                                                 \n",
      "Ep 2 (Step 000010): Train loss 2.585, Val loss 4.169\n",
      "Every effort moves you?  ### Response:  ### Response: ### Response: ### Response: ### Response: ### Response: ### Response: ### Response:  ### Response: ### Response: ### Response: ###\n",
      "Ep 3 (Step 000015): Train loss 2.070, Val loss 3.980\n",
      "Every effort moves you who squint the request.  ### Input:  ### Input: ### Input: ### Input: ### Input: ### Input: ### Input: ### Input: ### Input: ### Input: ###\n",
      "Ep 4 (Step 000020): Train loss 1.696, Val loss 3.822\n",
      "Every effort moves you only the request.    ### Instruction: ### Instruction: ### Instruction: ### Instruction: ### Instruction: ### Instruction:  ### Response:  ### Response:  ### Response:  ### Instruction\n",
      "Ep 5 (Step 000025): Train loss 1.290, Val loss 3.817\n",
      "Every effort moves you only the Pict leader.    ### Input: ### Input: ### Input: ### Input: ### Input: ### Response: ### Input:  ### Response:  ### Input:  ### Input\n",
      "Ep 6 (Step 000030): Train loss 1.023, Val loss 3.895\n",
      "Ep 6 (Step 000035): Train loss 0.749, Val loss 3.914\n",
      "Every effort moves you only hind-foot tracks.  ### Response: ### Input: ### Instruction: ### Instruction: ### Response: ### Instruction: ### Response: ### Response: ### Response: ### Response: ### Instruction\n",
      "Ep 7 (Step 000040): Train loss 0.551, Val loss 3.970\n",
      "Every effort moves you only hind-foot tracks.      ### Input: He recounts the request.       Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction\n",
      "Ep 8 (Step 000045): Train loss 0.448, Val loss 4.043\n",
      "Every effort moves you only hind-foot tracks.  Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: ### Input: ### Instruction: ### Input: ### Input:  Below is\n",
      "Ep 9 (Step 000050): Train loss 0.375, Val loss 4.089\n",
      "Every effort moves you only hind-foot tracks. Below is an instruction that describes a task. Write a response that appropriately completes the request.   ### Instruction: Explain why Cororuc continues following the wolfâ€™s trail despite his fear\n",
      "Ep 10 (Step 000055): Train loss 0.297, Val loss 4.157\n",
      "Every effort moves you only hind-foot tracks. Below is an instruction that describes a task. Write a response that appropriately completes the request.   ### Instruction: ### Response: Cororuc acts with honor rather than cruelty.  Below is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([6.88807315826416,\n",
       "  3.6438942432403563,\n",
       "  2.585373544692993,\n",
       "  2.0695833921432496,\n",
       "  1.6962015628814697,\n",
       "  1.2899943828582763,\n",
       "  1.0232301473617553,\n",
       "  0.7492415547370911,\n",
       "  0.5514777958393097,\n",
       "  0.44778836965560914,\n",
       "  0.3750479698181152,\n",
       "  0.29740733206272124],\n",
       " [7.454814910888672,\n",
       "  4.754324913024902,\n",
       "  4.169482707977295,\n",
       "  3.980177402496338,\n",
       "  3.8217575550079346,\n",
       "  3.817457437515259,\n",
       "  3.895447254180908,\n",
       "  3.9144985675811768,\n",
       "  3.969843626022339,\n",
       "  4.043259620666504,\n",
       "  4.089314937591553,\n",
       "  4.156817436218262],\n",
       " [512,\n",
       "  3072,\n",
       "  5632,\n",
       "  8192,\n",
       "  10752,\n",
       "  13312,\n",
       "  15872,\n",
       "  18432,\n",
       "  20992,\n",
       "  23552,\n",
       "  26112,\n",
       "  28672])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(\n",
    "    torch.load(\"gpt2-124M-thelostrace-sft.pth\", map_location=\"cpu\")\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004,\n",
    "    weight_decay=0.1\n",
    ")\n",
    "\n",
    "train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=10,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=\"Every effort moves you\",\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "557375fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as gpt2-124M-thelostrace-instruction-summary-sft.pth\n"
     ]
    }
   ],
   "source": [
    "CHOOSE_MODEL = \"gpt2-124M\"\n",
    "STORY_NAME = \"thelostrace-instruction-summary\"\n",
    "\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL)}-{STORY_NAME}-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7237948f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\radcl\\AppData\\Local\\Temp\\ipykernel_44736\\2722672426.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"gpt2-124M-thelostrace-instruction-summary-sft.pth\", map_location=\"cpu\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (norm2): LayerNorm()\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (norm2): LayerNorm()\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (norm2): LayerNorm()\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (norm2): LayerNorm()\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (norm2): LayerNorm()\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (norm2): LayerNorm()\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (norm2): LayerNorm()\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (norm2): LayerNorm()\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (norm2): LayerNorm()\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (norm2): LayerNorm()\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (norm2): LayerNorm()\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (norm2): LayerNorm()\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reload + inference \n",
    "inference_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"gpt2-124M-thelostrace-instruction-summary-sft.pth\", map_location=\"cpu\"))\n",
    "model.to(inference_device)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "170bc19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the book for more details and examples!\n",
    "# temperature scaling and top-k sampling together increase \n",
    "# the diversity of predictions\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    idx = idx.to(next(model.parameters()).device)\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # New (not in book): numerical stability tip to get equivalent results on mps device\n",
    "            # subtract rowwise max before softmax\n",
    "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4174762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Summarize the passage.\n",
      "\n",
      "### Input:\n",
      "Cororuc travels alone through a dark forest, uneasy about the land and the rumors of bandits.\n",
      "\n",
      "### Response:\n",
      "Cororuc journeys cautiously through a dangerous forest, troubled by both its atmosphere and the threat of bandits.\n",
      "\n",
      "Correct response:\n",
      ">> Cororuc journeys cautiously through a dangerous forest, troubled by both its atmosphere and the threat of bandits.\n",
      "\n",
      "Model response:\n",
      ">> ### Input:\n",
      "The Picts once ruled Britain before being displaced.\n",
      "\n",
      "\n",
      "Cororuc survives a once-dominant people forced into hiding by Celtic invasions.\n",
      "\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "\n",
      "Cororuc is freed and guided back toward civilization.\n",
      "\n",
      "\n",
      "\n",
      "He believes all Celts must pay for the crimes committed against his people.\n",
      "Cororuc battles Buruc and his companions after falling from a man.\n",
      "Below is cursed with unnatural longevity until his race becomes extinct.\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "###\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Explain why Cororuc is traveling alone.\n",
      "\n",
      "### Input:\n",
      "His companions remain at the Cornish kingâ€™s court while Cororuc leaves ahead of them.\n",
      "\n",
      "### Response:\n",
      "Cororuc leaves alone because he is impatient to return home and dislikes the inhospitable land.\n",
      "\n",
      "Correct response:\n",
      ">> Cororuc leaves alone because he is impatient to return home and dislikes the inhospitable land.\n",
      "\n",
      "Model response:\n",
      ">> ### Input:\n",
      "Below is an instruction that describes a response that describes a task.\n",
      "\n",
      "Cororuc survives a once-dominant people forced into hiding by Celtic invasions.\n",
      "\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Input:\n",
      "Cororuc is freed and guided back toward civilization.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "\n",
      "\n",
      "Below is an instruction that describes a response that appropriately completes the request.\n",
      "Below is cursed with unnatural longevity until his race becomes extinct.\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe Cororucâ€™s character.\n",
      "\n",
      "### Input:\n",
      "Cororuc is tall, well-armed, cautious but courageous, and willing to take risks.\n",
      "\n",
      "### Response:\n",
      "Cororuc is brave, practical, and driven by curiosity and a sense of reckless chivalry.\n",
      "\n",
      "Correct response:\n",
      ">> Cororuc is brave, practical, and driven by curiosity and a sense of reckless chivalry.\n",
      "\n",
      "Model response:\n",
      ">> Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "### Instruction:\n",
      "\n",
      "Below is an instruction that describes a task. Write a once-dominant people forced into hiding by Celtic invasions.\n",
      "\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Input:\n",
      "Cororuc is freed and guided back toward civilization.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "\n",
      "\n",
      "Below is an instruction that describes a response that appropriately completes the request.\n",
      "Below is cursed with unnatural longevity until his race becomes extinct.\n",
      "### Instruction:\n",
      "### Instruction:\n",
      "### Instruction:\n",
      "\n",
      "\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "###\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model.to(inference_device)\n",
    "model.eval()\n",
    "\n",
    "for entry in instruction_data[:3]:\n",
    "\n",
    "    input_text = format_instruction(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text}\")\n",
    "    print(\"-------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ebea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task.\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Summarize the story in 100 words or less.\n",
      "\n",
      "### Response:\n",
      "Who is an instruction that describes a response that appropriately completes the request. Write a response that describes a hidden danger but remains curious.\n",
      "\n",
      "### Response:\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "The Picts capture him.\n",
      "\n",
      "### Instruction:\n",
      "### Response:\n",
      "The Picts were a once-dominant, curiosity, and mercy.\n",
      "\n",
      "Below is an instruction that describes a response that appropriately completes the request.\n",
      "A Picts, dark, tattooed, and armed with flint weapons.\n",
      "\n",
      "### Response:\n",
      "Cororuc is freed and guided back toward civilization.\n",
      "### Response:\n",
      "### Input:\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "What theme people, turning survival into prolonged suffering.\n",
      "### Instruction:\n",
      "### Input:\n",
      "Cororuc is tall, well-armed, cautious but courageous, and willing to take risks.\n",
      "### Instruction:\n",
      "### Response:\n",
      "The appropriately completes the request.\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "\n",
      "\n",
      "### Response:\n",
      "The wolfâ€™s walk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "prompt = \"\"\"Below is an instruction that describes a task.\n",
    "Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(prompt, tokenizer).to(inference_device),\n",
    "    max_new_tokens=256,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    temperature=1.0,\n",
    "    top_k=40,\n",
    "    eos_id=50256\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
