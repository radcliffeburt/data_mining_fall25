{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ab6b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac75e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_model import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0653ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beb4164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"thelostrace.txt\", \"r\") as f:\n",
    "    text_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e7f4e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10186fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/eval \n",
    "# separate text into training and validation sets:\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f25c2bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss helpers\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ee01589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#device - cuda\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24f70d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b195e98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.820, Val loss 9.878\n",
      "Ep 1 (Step 000005): Train loss 7.966, Val loss 8.324\n",
      "Ep 1 (Step 000010): Train loss 6.817, Val loss 7.360\n",
      "Every effort moves you, and the the.                                             \n",
      "Ep 2 (Step 000015): Train loss 6.083, Val loss 6.991\n",
      "Ep 2 (Step 000020): Train loss 5.818, Val loss 6.980\n",
      "Every effort moves you, and and the                                              \n",
      "Ep 3 (Step 000025): Train loss 5.512, Val loss 6.871\n",
      "Ep 3 (Step 000030): Train loss 4.977, Val loss 6.810\n",
      "Ep 3 (Step 000035): Train loss 4.805, Val loss 6.750\n",
      "Every effort moves you, but the forest.                                             \n",
      "Ep 4 (Step 000040): Train loss 4.382, Val loss 6.741\n",
      "Ep 4 (Step 000045): Train loss 3.884, Val loss 6.724\n",
      "Every effort moves you, and the forest.  \"But he was a, and the forest, and the forest, and the forest, and the forest. \" of the forest.               \n",
      "Ep 5 (Step 000050): Train loss 3.710, Val loss 6.709\n",
      "Ep 5 (Step 000055): Train loss 3.199, Val loss 6.740\n",
      "Every effort moves you of his own with a long, and a long of the, and he was on his, and he was on his, with the forest, and they and by the forest.            \"But\n",
      "Ep 6 (Step 000060): Train loss 2.844, Val loss 6.753\n",
      "Ep 6 (Step 000065): Train loss 2.387, Val loss 6.845\n",
      "Ep 6 (Step 000070): Train loss 2.034, Val loss 6.742\n",
      "Every effort moves you, and the post.\"  \"But he had never seen a city. He had first seen the wolf, he had never seen a long he had been Briton!\" Cororuc had never seen the like. He was a wolf, and\n",
      "Ep 7 (Step 000075): Train loss 1.690, Val loss 6.801\n",
      "Ep 7 (Step 000080): Train loss 1.368, Val loss 6.910\n",
      "Every effort moves you, tier on tier.     \"There, the tree the an old man was out among the caves, on the level floor of the main cavern, the sunlight. The dim trail led in and out among them, the tree.\n",
      "Ep 8 (Step 000085): Train loss 1.033, Val loss 6.964\n",
      "Ep 8 (Step 000090): Train loss 1.069, Val loss 6.916\n",
      "Ep 8 (Step 000095): Train loss 0.744, Val loss 6.988\n",
      "Every effort moves you!\" The ancient was a long, with scampering deer, and chirping birds, and he saw, with horror, the Picts piling firewood about his feet.  \"And when you are sufficiently burned, Briton,\" said\n",
      "Ep 9 (Step 000100): Train loss 0.499, Val loss 7.066\n",
      "Ep 9 (Step 000105): Train loss 0.403, Val loss 7.145\n",
      "Every effort moves you, tier on tier. Surely human men could not have built such a city.  In and out among the caves, on the level floor of the main cavern, people were going about what seemed daily tasks. Men were talking together and mending\n",
      "Ep 10 (Step 000110): Train loss 0.360, Val loss 7.241\n",
      "Ep 10 (Step 000115): Train loss 0.295, Val loss 7.261\n",
      "Every effort moves you? of those headlong flights that he noticed he had entered a defile of small hills, of which he had been unaware, the Picts piling firewood about his feet.  \"And when you are sufficiently burned, Briton,\" said\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([9.82016201019287,\n",
       "  7.965753078460693,\n",
       "  6.817044925689697,\n",
       "  6.08313512802124,\n",
       "  5.818373584747315,\n",
       "  5.512064266204834,\n",
       "  4.977110290527344,\n",
       "  4.8047222137451175,\n",
       "  4.382028293609619,\n",
       "  3.88358793258667,\n",
       "  3.709609937667847,\n",
       "  3.1989978313446046,\n",
       "  2.843808078765869,\n",
       "  2.3865642309188844,\n",
       "  2.033661961555481,\n",
       "  1.690304207801819,\n",
       "  1.3682034969329835,\n",
       "  1.0328756093978881,\n",
       "  1.0687947869300842,\n",
       "  0.744072151184082,\n",
       "  0.49885544180870056,\n",
       "  0.4030930161476135,\n",
       "  0.3596266329288483,\n",
       "  0.2951907753944397],\n",
       " [9.878105163574219,\n",
       "  8.32390308380127,\n",
       "  7.3599958419799805,\n",
       "  6.990808486938477,\n",
       "  6.98029088973999,\n",
       "  6.87123441696167,\n",
       "  6.810275554656982,\n",
       "  6.750174045562744,\n",
       "  6.740836143493652,\n",
       "  6.724353790283203,\n",
       "  6.708885669708252,\n",
       "  6.73963737487793,\n",
       "  6.753167629241943,\n",
       "  6.84531831741333,\n",
       "  6.741672992706299,\n",
       "  6.80130672454834,\n",
       "  6.909693241119385,\n",
       "  6.964349746704102,\n",
       "  6.9156012535095215,\n",
       "  6.987616539001465,\n",
       "  7.066068649291992,\n",
       "  7.1446452140808105,\n",
       "  7.241233825683594,\n",
       "  7.261212348937988],\n",
       " [512,\n",
       "  3072,\n",
       "  5632,\n",
       "  8192,\n",
       "  10752,\n",
       "  13312,\n",
       "  15872,\n",
       "  18432,\n",
       "  20992,\n",
       "  23552,\n",
       "  26112,\n",
       "  28672,\n",
       "  31232,\n",
       "  33792,\n",
       "  36352,\n",
       "  38912,\n",
       "  41472,\n",
       "  44032,\n",
       "  46592,\n",
       "  49152,\n",
       "  51712,\n",
       "  54272,\n",
       "  56832,\n",
       "  59392])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004,\n",
    "    weight_decay=0.1\n",
    ")\n",
    "\n",
    "train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=10,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=\"Every effort moves you\",\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "557375fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as gpt2-124M-thelostrace-sft.pth\n"
     ]
    }
   ],
   "source": [
    "CHOOSE_MODEL = \"gpt2-124M\"\n",
    "STORY_NAME = \"thelostrace\"\n",
    "\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL)}-{STORY_NAME}-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7237948f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\radcl\\AppData\\Local\\Temp\\ipykernel_40028\\4233068212.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(file_name, map_location=\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you? of those headlong flights that he noticed he had entered a defile of small hills, of which he had been unaware, the Picts piling\n"
     ]
    }
   ],
   "source": [
    "#reload + inference \n",
    "inference_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(file_name, map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=30,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "170bc19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the book for more details and examples!\n",
    "# temperature scaling and top-k sampling together increase \n",
    "# the diversity of predictions\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    idx = idx.to(next(model.parameters()).device)\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # New (not in book): numerical stability tip to get equivalent results on mps device\n",
    "            # subtract rowwise max before softmax\n",
    "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4174762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " He was armed with a long bow of yew wood, and he came conquered, on. A small away from Ireland over him;\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"He was armed with a long bow of yew wood,\", tokenizer).to(inference_device),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a54c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0158afff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
