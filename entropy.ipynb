{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a577d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57edbbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4320c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make training faster on a laptop, change context_length as shown:\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension \n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate (10% of activation tensor is set to 0, per forward pass)\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16b5282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "744511e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f26be878",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"He was armed with a long bow of yew wood\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eceda77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " He was armed with a long bow of yew woodoren eloquvisual battleenda oralTal mediation Valve specs\n"
     ]
    }
   ],
   "source": [
    "#to give gibberish output just to show its trying\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d589e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"He was armed\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" armed with a\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3366315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): #disables gradient tracking, pytorch builds no graph and no gradient is stored. \n",
    "    #used for inference/eval used for running the model not learning\n",
    "    logits = model(inputs)\n",
    "\n",
    "probs = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probs.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "016d2404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.7697e-05, 9.0636e-06, 3.1339e-05,  ..., 8.1393e-06,\n",
       "          9.5565e-06, 2.2934e-05],\n",
       "         [2.0467e-05, 1.1062e-05, 7.1092e-06,  ..., 7.0050e-06,\n",
       "          2.8771e-05, 1.2013e-05],\n",
       "         [2.2424e-05, 6.8287e-06, 1.9796e-05,  ..., 9.0208e-06,\n",
       "          1.3735e-05, 1.5140e-05]],\n",
       "\n",
       "        [[1.2894e-05, 1.3995e-05, 4.0670e-05,  ..., 9.9802e-06,\n",
       "          1.5458e-05, 1.6000e-05],\n",
       "         [2.2501e-05, 1.0211e-05, 1.1247e-05,  ..., 1.2867e-05,\n",
       "          1.5972e-05, 1.2637e-05],\n",
       "         [2.1587e-05, 6.2733e-06, 3.3350e-05,  ..., 1.1212e-05,\n",
       "          1.7629e-05, 1.3616e-05]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02286211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[26711],\n",
      "         [ 6557],\n",
      "         [46612]],\n",
      "\n",
      "        [[34487],\n",
      "         [13791],\n",
      "         [33070]]])\n"
     ]
    }
   ],
   "source": [
    "# predicted tokens:\n",
    "token_ids = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)\n",
    "\n",
    "#picks highest probable token at each position, dim -1 -> across dimension\n",
    "# keep dim- the shape remains aligned, it stays able to get a token, predict then keep shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e208aa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 2:  really like chocolate\n",
      "Outputs batch 2: Pri grey cram\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 2: {token_ids_to_text(targets[1], tokenizer)}\")\n",
    "print(f\"Outputs batch 2: {token_ids_to_text(token_ids[1].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d39e34eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3626,  6100,   345],\n",
       "        [ 1107,   588, 11311]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e1274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([2.1299e-05, 1.7519e-05, 1.5568e-05])\n",
      "Text 2: tensor([2.0574e-05, 1.8915e-05, 1.0504e-05])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probs_1 = probs[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probs_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probs_2 = probs[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probs_2)\n",
    "\n",
    "# selecting models predicted probabilities for correct next tokens\n",
    "# probs = shape(b,seq, vocab), that shape is the same as batch, num_tokens, d_in \n",
    "# for each token position, give probability, for each token, give me vector representation \n",
    "#  text \n",
    "#batch of 2, seq_len = 3, each row = 1 text sample\n",
    "# #[\n",
    "#   [t11, t12, t13],\n",
    "#   [t21, t22, t23]\n",
    "# ]#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad6b54ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions( sci_mode=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18344499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.7568, -10.9522, -11.0703, -10.7915, -10.8755, -11.4638])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probs = torch.log(torch.cat((target_probs_1, target_probs_2)))\n",
    "print(log_probs)\n",
    "\n",
    "#cross entropy calcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b08df10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.9850)\n",
      "tensor(10.9850)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probs = torch.mean(log_probs)\n",
    "print(avg_log_probs)\n",
    "neg_avg_log_probs = avg_log_probs * -1\n",
    "print(neg_avg_log_probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc539f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)\n",
    "\n",
    "# flatten reshapes so cross entropy treats every token prediction as one \n",
    "#classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d014ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1166,  0.4131,  0.1192,  ..., -0.9238, -0.4077, -0.2832],\n",
      "        [ 0.3386, -0.0832,  0.8338,  ..., -0.7654, -0.0748, -0.6247],\n",
      "        [ 0.3640,  0.0377,  1.0781,  ...,  0.7329, -0.1739, -0.1215],\n",
      "        [-0.3704,  0.9823,  0.9950,  ..., -0.7827, -0.9639, -0.5060],\n",
      "        [ 0.4205,  0.6336,  0.2788,  ..., -0.3413, -0.1324, -0.0769],\n",
      "        [-0.1156,  0.4582,  1.3515,  ...,  0.0808, -0.3733,  0.4753]])\n",
      "tensor([ 3626,  6100,   345,  1107,   588, 11311])\n"
     ]
    }
   ],
   "source": [
    "print(logits_flat)\n",
    "print(targets_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd4496e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.9850)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da356f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(58984.5117)\n"
     ]
    }
   ],
   "source": [
    "# a more interpretable version of cross entropy --\n",
    "# this is basically the number of tokens the model considers for predicted output\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)\n",
    "\n",
    "# the model is about as sure as we have 59000 possible predicted tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8be5a53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cororuc glanced about him and hastened his pace. He was no coward, but he did not like the place. Ta\n"
     ]
    }
   ],
   "source": [
    "# use the short story from before for training:\n",
    "with open( \"thelostrace.txt\", \"r\" ) as f:\n",
    "    text_data = f.read()\n",
    "print(text_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ec615ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51a99344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate text into training and validation sets:\n",
    "# Train/validation ratio\n",
    "\n",
    "\n",
    "train_ratio = 0.90 # using 90% of the data to train on, 10% to validate or to \"check\" how the model does\n",
    "split_idx = int(train_ratio * len(text_data)) # index where you split the data from train to val\n",
    "train_data = text_data[:split_idx] \n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123) #makes results reproducible \n",
    "\n",
    "train_loader = create_dataloader_v1(          # build data loader for training\n",
    "    train_data,                               # training text data\n",
    "    batch_size=2,                             # 2 sequences per batch\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],  # max tokens per sequence\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],      # move window by full length\n",
    "    drop_last=True,                           # drop incomplete last batch\n",
    "    shuffle=True,                             # randomize order each epoch\n",
    "    num_workers=0                             # load data in main process\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(            # build data loader for validation\n",
    "    val_data,                                 # validation text data\n",
    "    batch_size=2,                             # 2 sequences per batch\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],  # same context length\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],      # same window step\n",
    "    drop_last=False,                          # keep smaller last batch\n",
    "    shuffle=False,                            # keep order fixed\n",
    "    num_workers=0                             # load data in main process\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "170dd75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 6144\n",
      "Validation tokens: 512\n",
      "All tokens: 6656\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0                              # counter for training tokens\n",
    "for input_batch, target_batch in train_loader:  # loop over training batches\n",
    "    train_tokens += input_batch.numel()       # count all tokens in batch\n",
    "\n",
    "val_tokens = 0                                # counter for validation tokens\n",
    "for input_batch, target_batch in val_loader:  # loop over validation batches\n",
    "    val_tokens += input_batch.numel()         # count all tokens in batch\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)       # show training token count\n",
    "print(\"Validation tokens:\", val_tokens)       # show validation token count\n",
    "print(\"All tokens:\", train_tokens + val_tokens)  # show total tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4eb08b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):   # compute loss for one batch\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)  # move to CPU/GPU\n",
    "    logits = model(input_batch)                                   # forward pass → predictions\n",
    "    loss = torch.nn.functional.cross_entropy(                     # compute CE loss\n",
    "        logits.flatten(0, 1),                                      # merge batch + time dims\n",
    "        target_batch.flatten()                                     # flatten targets to match\n",
    "    )\n",
    "    return loss                                                    # return batch loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):  # avg loss over loader\n",
    "    total_loss = 0.                                                 # accumulate loss\n",
    "    if len(data_loader) == 0:                                       # handle empty loader\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:                                       # use all batches\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))            # cap batch count\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):   # loop over batches\n",
    "        if i < num_batches:                                         # stop at limit\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)  # batch loss\n",
    "            total_loss += loss.item()                               # add scalar loss\n",
    "        else:\n",
    "            break                                                   # exit loop\n",
    "    return total_loss / num_batches                                  # return mean loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf190bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n",
      "Training loss: 10.98396583557129\n",
      "Validation loss: 10.992756207784018\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():                 # check if NVIDIA GPU is available\n",
    "    device = torch.device(\"cuda\")             # use CUDA GPU\n",
    "elif torch.backends.mps.is_available():       # check if Apple GPU is available\n",
    "    major, minor = map(int, torch.__version__.split(\".\")[:2])  # get PyTorch version\n",
    "    if (major, minor) >= (2, 9):               # ensure version supports stable MPS\n",
    "        device = torch.device(\"mps\")           # use Apple GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")               # fallback to CPU\n",
    "\n",
    "print(f\"Using {device} device.\")               # print chosen device\n",
    "\n",
    "\n",
    "model.to(device)                              # move model to selected device\n",
    "\n",
    "torch.manual_seed(123)                        # fix randomness for repeatable results\n",
    "\n",
    "with torch.no_grad():                         # disable gradients (no training)\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)  # compute train loss\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)      # compute val loss\n",
    "\n",
    "print(\"Training loss:\", train_loss)           # print training loss\n",
    "print(\"Validation loss:\", val_loss)           # print validation loss\n",
    "\n",
    "\"\"\"\n",
    "**the model isn’t learning anything yet**.\n",
    "\n",
    "* Train ≈ Val → no overfitting\n",
    "* Loss = **very high** → predictions are mostly wrong\n",
    "* Model is near **random guessing**\n",
    "* Typical causes: untrained weights, tiny dataset, or wrong targets\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff6c860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53faf58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):  \n",
    "    # Trains the model and periodically evaluates performance\n",
    "\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []  \n",
    "    # Lists to store loss history and how many tokens the model has seen\n",
    "\n",
    "    tokens_seen, global_step = 0, -1  \n",
    "    # Counters for total tokens processed and total optimization steps\n",
    "\n",
    "    for epoch in range(num_epochs):  \n",
    "        # Loop over full passes through the training dataset\n",
    "        model.train()  \n",
    "        # Enable training behavior (dropout on, etc.)\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:  \n",
    "            # Iterate over batches of tokenized input/target pairs\n",
    "\n",
    "            optimizer.zero_grad()  \n",
    "            # Clear gradients from the previous batch\n",
    "\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)  \n",
    "            # Forward pass + compute cross-entropy loss for this batch\n",
    "\n",
    "            loss.backward()  \n",
    "            # Backpropagate loss to compute gradients for all parameters\n",
    "\n",
    "            optimizer.step()  \n",
    "            # Update model weights using the computed gradients\n",
    "\n",
    "            tokens_seen += input_batch.numel()  \n",
    "            # Count how many tokens have been processed so far\n",
    "\n",
    "            global_step += 1  \n",
    "            # Count how many optimization steps have occurred\n",
    "\n",
    "            if global_step % eval_freq == 0:  \n",
    "                # Periodically evaluate model performance\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )  \n",
    "                # Compute average train/validation loss on a subset of batches\n",
    "\n",
    "                train_losses.append(train_loss)  \n",
    "                # Save training loss\n",
    "\n",
    "                val_losses.append(val_loss)  \n",
    "                # Save validation loss\n",
    "\n",
    "                track_tokens_seen.append(tokens_seen)  \n",
    "                # Track learning progress vs tokens seen\n",
    "\n",
    "                print(\n",
    "                    f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\"\n",
    "                )  \n",
    "                # Print progress update\n",
    "\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )  \n",
    "        # Generate sample text to qualitatively inspect learning progress\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen  \n",
    "    # Return loss history and token counts\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):  \n",
    "    # Evaluates model loss without updating weights\n",
    "\n",
    "    model.eval()  \n",
    "    # Switch to evaluation mode (dropout off)\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        # Disable gradient tracking for speed and memory savings\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )  \n",
    "        # Compute average training loss on limited batches\n",
    "\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )  \n",
    "        # Compute average validation loss on limited batches\n",
    "\n",
    "    model.train()  \n",
    "    # Switch back to training mode\n",
    "\n",
    "    return train_loss, val_loss  \n",
    "    # Return evaluation losses\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):  \n",
    "    # Generates example text from the current model state\n",
    "\n",
    "    model.eval()  \n",
    "    # Set model to inference mode\n",
    "\n",
    "    context_size = model.pos_emb.weight.shape[0]  \n",
    "    # Get maximum context length from positional embeddings\n",
    "\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)  \n",
    "    # Tokenize the starting text and move it to the device\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        # No gradients needed during generation\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model,\n",
    "            idx=encoded,\n",
    "            max_new_tokens=50,\n",
    "            context_size=context_size\n",
    "        )  \n",
    "        # Autoregressively generate new tokens\n",
    "\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)  \n",
    "    # Convert generated token IDs back to readable text\n",
    "\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  \n",
    "    # Print generated text in a compact format\n",
    "\n",
    "    model.train()  \n",
    "    # Restore training mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4661c458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.820, Val loss 9.878\n",
      "Ep 1 (Step 000005): Train loss 7.966, Val loss 8.324\n",
      "Ep 1 (Step 000010): Train loss 6.817, Val loss 7.360\n",
      "He was armed.                                                 \n",
      "Ep 2 (Step 000015): Train loss 6.083, Val loss 6.991\n",
      "Ep 2 (Step 000020): Train loss 5.818, Val loss 6.980\n",
      "He was armed, and, and the                                             \n",
      "Ep 3 (Step 000025): Train loss 5.512, Val loss 6.871\n",
      "Ep 3 (Step 000030): Train loss 4.977, Val loss 6.810\n",
      "Ep 3 (Step 000035): Train loss 4.805, Val loss 6.750\n",
      "He was armed. The a great a.                                            \n",
      "Ep 4 (Step 000040): Train loss 4.382, Val loss 6.741\n",
      "Ep 4 (Step 000045): Train loss 3.884, Val loss 6.724\n",
      "He was armed, and a great to the forest, and the forest.          \" was in the forest, and the forest, and the forest, and the forest, and the forest, and the forest.  \n",
      "Ep 5 (Step 000050): Train loss 3.710, Val loss 6.709\n",
      "Ep 5 (Step 000055): Train loss 3.199, Val loss 6.740\n",
      "He was armed with a race; and then for the forest. \"But, and he was not at the forest, and the forest, with the forest, he was not have been him and he had been a long in the forest, and he had been\n",
      "Ep 6 (Step 000060): Train loss 2.844, Val loss 6.753\n",
      "Ep 6 (Step 000065): Train loss 2.387, Val loss 6.845\n",
      "Ep 6 (Step 000070): Train loss 2.034, Val loss 6.742\n",
      "He was armed with a strange; a long bronze, and a strange a long, and he was a long bronze and he was a caves, with a long of a place, in his feet.  \"And when you are, \"they are short but\n",
      "Ep 7 (Step 000075): Train loss 1.690, Val loss 6.801\n",
      "Ep 7 (Step 000080): Train loss 1.368, Val loss 6.910\n",
      "He was armed with a longed for.     \"There, the tree, the post, the forest was on the trail he had been Briton, the Briton!\"  And following the Briton,\".     \n",
      "Ep 8 (Step 000085): Train loss 1.033, Val loss 6.964\n",
      "Ep 8 (Step 000090): Train loss 1.069, Val loss 6.916\n",
      "Ep 8 (Step 000095): Train loss 0.744, Val loss 6.988\n",
      "He was armed with a small of the stone seat, like a throne; and in it.   The ancient, \" made boats and set sail for the men, and it was in his pursuers had never seen the like.  The ancient was\n",
      "Ep 9 (Step 000100): Train loss 0.499, Val loss 7.066\n",
      "Ep 9 (Step 000105): Train loss 0.403, Val loss 7.145\n",
      "He was armed with a small; some said that they still lurked in the hill of Sil, an old woman witch cursed me as she writhed at the stake.  \"But the sunlight. The dim trail led in the cavern, but to the fullest\n",
      "Ep 10 (Step 000110): Train loss 0.360, Val loss 7.241\n",
      "Ep 10 (Step 000115): Train loss 0.295, Val loss 7.261\n",
      "He was armed with a heard burst of savage yells, from back up the trail, and chirping birds, of which he was used. He longed for the fight and had found him gone. The dim trail led in and out among them, sometimes sk\n"
     ]
    }
   ],
   "source": [
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"He was armed\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "# end_time = time.time()\n",
    "# execution_time_minutes = (end_time - start_time) / 60\n",
    "# print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c992ca61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02a677d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " He was armed with a heard burst of savage yells, from back up the trail, and chirping birds, of which he was used\n"
     ]
    }
   ],
   "source": [
    "inference_device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(inference_device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"He was armed\", tokenizer).to(inference_device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a2fd6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the book for more details and examples!\n",
    "# temperature scaling and top-k sampling together increase \n",
    "# the diversity of predictions\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # New (not in book): numerical stability tip to get equivalent results on mps device\n",
    "            # subtract rowwise max before softmax\n",
    "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5aad5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " He was armed that far two kings! long that if the and miles it vanish into our\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"He was armed\", tokenizer).to(inference_device),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c25944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
