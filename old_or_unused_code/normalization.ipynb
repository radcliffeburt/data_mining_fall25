{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ed1465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\radcl\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_subclasses\\functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a0763e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5778,  1.6956, -1.9398, -0.6370,  0.3499,  0.9727],\n",
       "        [-0.9691, -1.4902,  0.8478, -1.4404,  0.0469, -0.9731],\n",
       "        [ 1.0857,  0.8106, -0.1296,  0.9529, -0.5721, -1.0813]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = torch.randn(3,6)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c902ff9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3365],\n",
       "        [-0.6630],\n",
       "        [ 0.1777]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = outputs.mean( dim=-1, keepdim=True )\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85fc1959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4087],\n",
       "        [0.9236],\n",
       "        [0.9019]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd = outputs.std( dim=-1, keepdim=True)\n",
    "sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "493a5963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8811,  0.9648, -1.6159, -0.6911,  0.0095,  0.4516],\n",
       "        [-0.3314, -0.8956,  1.6359, -0.8417,  0.7686, -0.3357],\n",
       "        [ 1.0067,  0.7017, -0.3407,  0.8594, -0.8313, -1.3959]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_outputs = (outputs - mean)/sd\n",
    "normalized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38667282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.9341e-09],\n",
       "        [ 9.9341e-09],\n",
       "        [ 0.0000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_outputs.mean( dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "218e5ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_outputs.std( dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "814efb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions( sci_mode=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2509559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d0d443",
   "metadata": {},
   "source": [
    "This file creates the normalizing layer class. This turns all token emb vectors from really small numbers to between 0-1\n",
    "\n",
    "first chunk sets up parameters,\n",
    "1. embedding size(features per token)\n",
    "2. eps (a constant in this case .000005 to avoid undfeined vectors when dividing by zero)\n",
    "3. self scale and shift a learnable weight and bias that stretches and shifts normalized values\n",
    "    - scale: multiplies a normalized vector bt a learnable weight (stretching/compressing a dimension after normalization)\n",
    "    - shfit: allows the model to recenter the normalized output wherever it wants, not just zero\n",
    "\n",
    "* feature = tensor x entering the layer shaped as (batch_size, seq_length, emb_dim) *\n",
    "\n",
    "quick note: context_len: max num tokens the model can consider at once, vocab_size: total num unique tokens, seq_len :the actual num of tokens in a specific input sequence.\n",
    "\n",
    "\n",
    "Second chunk\n",
    "mean = computing the mean across features\n",
    "var = computing variance across same features\n",
    "norm = normalize each embedding to have mean 0 and var 1\n",
    "then returns a rescaled and shifted normalized output with learnable scale and shift parameters \n",
    "\n",
    "how this all fits in: \n",
    "\n",
    "Layer normalization makes every tokens featrure vector (b,seq_len, emb_dim) stable, learnable rescaled and ready for the mha that does all the attention scores, weighst to create context vects\n",
    "\n",
    "MHA then takes this bad boys and finds dot products, \"which vectors attends to which\"\n",
    "\n",
    "What happens if this normalization isnt done:\n",
    "\n",
    "vectors get blown up or vanish, gradients dont work and training becomes chaos. Starting with a clean slate before anything else is necessary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1302b500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
