{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c40502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63e40bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.1 Number of parameters in feed forward and attention modules\n",
    "# Calculate and compare the number of parameters that are contained in the feed forward module \n",
    "# and those that are contained in the multi-head attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e479d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_model import TransformerBlock\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4402ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBlock(\n",
      "  (norm1): LayerNorm()\n",
      "  (att): MultiHeadAttention(\n",
      "    (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (norm2): LayerNorm()\n",
      "  (feedforward): FeedForward(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (1): GELU()\n",
      "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "print(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "824aca87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters for feed forward module: 4,722,432\n"
     ]
    }
   ],
   "source": [
    "# total parameters in the feed forward mod\n",
    "\n",
    "total_params = sum(p.numel() for p in block.feedforward.parameters())\n",
    "print(f\"Total number of parameters for feed forward module: {total_params :,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad49be8",
   "metadata": {},
   "source": [
    "The calculations for  within the feed forward module:\n",
    "- 1st Linear layer: 768 inputs × 4×768 outputs + 4×768 bias units = 2,362,368\n",
    "- 2nd Linear layer: 4×768 inputs × 768 outputs + 768 bias units = 2,360,064\n",
    "- Total: 1st Linear layer + 2nd Linear layer = 2,362,368 + 2,360,064 = 4,722,432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1460e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in attention module: 2,360,064\n"
     ]
    }
   ],
   "source": [
    "#Paramters in the attention mod\n",
    "total_params = sum(p.numel() for p in block.att.parameters())\n",
    "print(f\"Total number of parameters in attention module: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab2f56c",
   "metadata": {},
   "source": [
    "Calculations for the attention mod\n",
    "\n",
    "- W_query: 768 inputs × 768 outputs = 589,824\n",
    "- W_key: 768 inputs × 768 outputs = 589,824\n",
    "- W_value: 768 inputs × 768 outputs = 589,824\n",
    "- out_proj: 768 inputs × 768 outputs + 768 bias units = 590,592\n",
    "- Total: W_query + W_key + W_value + out_proj = 3×589,824 + 590,592 = 2,360,064"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f855a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2 Initializing GPT models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dbf6fa",
   "metadata": {},
   "source": [
    "GPT2-small \n",
    "emb_dim = 768\n",
    "n_layers = 12\n",
    "n_heads = 12\n",
    "\n",
    "GPT2-medium\n",
    "emb_dim = 1024\n",
    "n_layers = 24\n",
    "n_heads = 16\n",
    "\n",
    "GPT2- Large:\n",
    "emb_dim = 1280\n",
    "n_layers = 24\n",
    "n_heads = 16\n",
    "\n",
    "GPT2-XL\n",
    "emb_dim = 1600\n",
    "n_layers = 12\n",
    "n_heads = 12\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42244cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_SMALL = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "GPT_CONFIG_MED = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 1024,\n",
    "    \"n_heads\": 16,\n",
    "    \"n_layers\": 24,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "GPT_CONFIG_LG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 1280,\n",
    "    \"n_heads\": 20,\n",
    "    \"n_layers\": 36,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "GPT_CONFIG_XL = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 1600,\n",
    "    \"n_heads\": 25,\n",
    "    \"n_layers\": 48,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d274ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"medium\": GPT_CONFIG_MED,\n",
    "    \"xl\" : GPT_CONFIG_XL\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc57257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_size(model): # based on chapter code\n",
    "    \n",
    "    total_params = total_params = sum(p.numel() for p in block.att.parameters())\n",
    "    print(f\"Total number of parameters in model: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddb62119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "GPT2-MEDIUM:\n",
      "Total number of parameters in model: 2,360,064\n",
      "\n",
      "\n",
      "GPT2-XL:\n",
      "Total number of parameters in model: 2,360,064\n"
     ]
    }
   ],
   "source": [
    "from gpt_model import GPTModel\n",
    "\n",
    "for model_abbrev in (\"medium\", \"xl\"):\n",
    "    model_name = f\"gpt2-{model_abbrev}\"\n",
    "    config = configs[model_abbrev]\n",
    "    model = GPTModel(config)\n",
    "    print(f\"\\n\\n{model_name.upper()}:\")\n",
    "    calculate_size(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
